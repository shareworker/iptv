#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IPTV æ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿä¸»è¿è¡Œå™¨ - Main Runner
æ•´åˆæ‰€æœ‰ä¼˜åŒ–æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„ç›´æ’­æºä¼˜åŒ–æµç¨‹
"""

import os
import sys
import json
import asyncio
import logging
import argparse
import time
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sources_dir = os.path.join(current_dir, 'sources')
latency_dir = os.path.join(current_dir, 'latency')
sys.path.append(sources_dir)
sys.path.append(latency_dir)

from source_aggregator import SourceAggregator
from latency_tester import LatencyTester
from smart_optimizer import SmartOptimizer
from tvbox_optimizer import TVBoxOptimizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimization_runner.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ ç›´æ¥æ‰“å°å‡½æ•°
def print_log(message):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")
    sys.stdout.flush()

class OptimizationRunner:
    """ä¼˜åŒ–ç³»ç»Ÿä¸»è¿è¡Œå™¨"""

    def __init__(self, args=None):
        # è®°å½•å¯åŠ¨æ—¶é—´
        self.start_time = datetime.now().isoformat()
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        if args is None:
            args = self._parse_args()
        
        self.config_dir = args.config_dir
        self.output_base = args.output_dir
        self.output_dir = os.path.join(self.output_base, "enhanced_playlists")
        self.cache_dir = os.path.join(self.config_dir, "cache")
        self.batch_size = args.batch_size
        self.test_mode = args.test_mode
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.cache_dir, exist_ok=True)
        
        logger.info("ä¼˜åŒ–ç³»ç»Ÿä¸»è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        
    def _parse_args(self):
        """è§£æå‘½ä»¤è¡Œå‚æ•°"""
        parser = argparse.ArgumentParser(description="IPTVæ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿ")
        
        parser.add_argument(
            "--config-dir", 
            default="sources",
            help="é…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º 'sources'"
        )
        
        parser.add_argument(
            "--output-dir", 
            default="output",
            help="è¾“å‡ºæ–‡ä»¶ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º 'output'"
        )
        
        parser.add_argument(
            "--batch-size", 
            type=int, 
            default=20,
            help="å»¶è¿Ÿæµ‹è¯•æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º 20"
        )
        
        parser.add_argument(
            "--test-mode", 
            action="store_true",
            help="æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®è€Œéå®é™…è·å–æº"
        )
        
        return parser.parse_args()

    async def run_full_optimization(self):
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æµç¨‹"""
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´çš„IPTVä¼˜åŒ–æµç¨‹...")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        process_start_time = time.time()

        try:
            # é˜¶æ®µ1: æºèšåˆ
            logger.info("=== é˜¶æ®µ1: æºèšåˆ ===")
            stage_start_time = time.time()
            
            # æµ‹è¯•æ¨¡å¼ä¸‹ç›´æ¥åŠ è½½æµ‹è¯•æ•°æ®
            if self.test_mode:
                logger.info("æµ‹è¯•æ¨¡å¼: åŠ è½½æµ‹è¯•æ•°æ®")
                test_data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'test_data.json')
                if os.path.exists(test_data_path):
                    with open(test_data_path, 'r', encoding='utf-8') as f:
                        test_data = json.load(f)
                        channels = test_data.get('test_channels', [])
                    logger.info(f"æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(channels)} ä¸ªé¢‘é“")
                else:
                    logger.warning(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
                    channels = []
            else:
                # æ­£å¸¸æ¨¡å¼ä¸‹è¿è¡Œæºèšåˆ
                channels = await self._run_source_aggregation()
                
            stage_end_time = time.time()
            logger.info(f"é˜¶æ®µ1å®Œæˆï¼Œè€—æ—¶: {stage_end_time - stage_start_time:.2f}ç§’")
            if not channels:
                logger.warning("æºèšåˆå¤±è´¥ï¼Œè·³è¿‡åç»­æ­¥éª¤")
                return

            # é˜¶æ®µ2: å»¶è¿Ÿæµ‹è¯•
            logger.info("=== é˜¶æ®µ2: å»¶è¿Ÿæµ‹è¯• ===")
            stage_start_time = time.time()
            tested_channels = await self._run_latency_testing(channels)
            stage_end_time = time.time()
            logger.info(f"é˜¶æ®µ2å®Œæˆï¼Œè€—æ—¶: {stage_end_time - stage_start_time:.2f}ç§’")
            if not tested_channels:
                logger.warning("å»¶è¿Ÿæµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                tested_channels = channels

            # é˜¶æ®µ3: æ™ºèƒ½ä¼˜åŒ–
            logger.info("=== é˜¶æ®µ3: æ™ºèƒ½ä¼˜åŒ– ===")
            stage_start_time = time.time()
            optimized_channels = self._run_smart_optimization(tested_channels)
            stage_end_time = time.time()
            logger.info(f"é˜¶æ®µ3å®Œæˆï¼Œè€—æ—¶: {stage_end_time - stage_start_time:.2f}ç§’")

            # é˜¶æ®µ4: TVBoxä¸“ç”¨ä¼˜åŒ–
            logger.info("=== é˜¶æ®µ4: TVBoxä¸“ç”¨ä¼˜åŒ– ===")
            stage_start_time = time.time()
            tvbox_channels = self._run_tvbox_optimization(optimized_channels)
            stage_end_time = time.time()
            logger.info(f"é˜¶æ®µ4å®Œæˆï¼Œè€—æ—¶: {stage_end_time - stage_start_time:.2f}ç§’")

            # é˜¶æ®µ5: ç”Ÿæˆæœ€ç»ˆç»“æœ
            logger.info("=== é˜¶æ®µ5: ç”Ÿæˆæœ€ç»ˆç»“æœ ===")
            stage_start_time = time.time()
            self._generate_final_output(optimized_channels, tvbox_channels)
            stage_end_time = time.time()
            logger.info(f"é˜¶æ®µ5å®Œæˆï¼Œè€—æ—¶: {stage_end_time - stage_start_time:.2f}ç§’")
            
            # è®¡ç®—æ€»è€—æ—¶
            process_end_time = time.time()
            total_duration = process_end_time - process_start_time
            logger.info(f"ğŸ‰ IPTVä¼˜åŒ–æµç¨‹å®Œæˆ! æ€»è€—æ—¶: {total_duration:.2f}ç§’")

        except Exception as e:
            logger.error(f"ä¼˜åŒ–æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _run_source_aggregation(self) -> List[Dict]:
        """è¿è¡Œæºèšåˆ"""
        logger.info("å¯åŠ¨æºèšåˆå™¨...")

        try:
            aggregator = SourceAggregator()
            channels = await aggregator.aggregate_sources()
            
            if not channels:
                logger.warning("æœªè·å–åˆ°ä»»ä½•é¢‘é“æ•°æ®ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®")
                # å¦‚æœæ²¡æœ‰è·å–åˆ°é¢‘é“ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®
                test_data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'test_data.json')
                if os.path.exists(test_data_path):
                    logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
                    with open(test_data_path, 'r', encoding='utf-8') as f:
                        test_data = json.load(f)
                        channels = test_data.get('test_channels', [])
                    logger.info(f"æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(channels)} ä¸ªé¢‘é“")

            # ä¿å­˜èšåˆç»“æœ
            # ä¿å­˜åˆ°åŸºç¡€è¾“å‡ºç›®å½•
            os.makedirs(self.output_base, exist_ok=True)
            aggregated_file = os.path.join(self.output_base, "aggregated_channels.json")
            with open(aggregated_file, 'w', encoding='utf-8') as f:
                json.dump(channels, f, ensure_ascii=False, indent=2)

            logger.info(f"æºèšåˆå®Œæˆï¼Œå…±è·å– {len(channels)} ä¸ªé¢‘é“")
            return channels
        except Exception as e:
            logger.error(f"æºèšåˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # å°è¯•åŠ è½½æµ‹è¯•æ•°æ®ä½œä¸ºå¤‡ä»½
            try:
                test_data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'test_data.json')
                if os.path.exists(test_data_path):
                    logger.info(f"åŠ è½½å¤‡ç”¨æµ‹è¯•æ•°æ®: {test_data_path}")
                    with open(test_data_path, 'r', encoding='utf-8') as f:
                        test_data = json.load(f)
                        channels = test_data.get('test_channels', [])
                    logger.info(f"å¤‡ç”¨æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(channels)} ä¸ªé¢‘é“")
                    return channels
            except Exception as backup_error:
                logger.error(f"åŠ è½½å¤‡ç”¨æµ‹è¯•æ•°æ®å¤±è´¥: {backup_error}")
            
            return []

    async def _run_latency_testing(self, channels: List[Dict]) -> List[Dict]:
        """è¿è¡Œå»¶è¿Ÿæµ‹è¯•"""
        logger.info(f"å¯åŠ¨å»¶è¿Ÿæµ‹è¯•ï¼Œå…± {len(channels)} ä¸ªé¢‘é“")

        if not channels:
            logger.warning("æ²¡æœ‰é¢‘é“æ•°æ®å¯ä¾›æµ‹è¯•")
            return []

        try:
            tester = LatencyTester()
            tested_channels = await tester.test_channels_batch(channels, batch_size=self.batch_size)

            # ä¿å­˜æµ‹è¯•ç»“æœ
            test_results_file = os.path.join(self.output_base, "latency_test_results.json")
            with open(test_results_file, 'w', encoding='utf-8') as f:
                json.dump(tested_channels, f, ensure_ascii=False, indent=2)

            # è¿‡æ»¤æœ‰æ•ˆç»“æœ
            valid_channels = [c for c in tested_channels if c.get('status') == 'success']
            logger.info(f"å»¶è¿Ÿæµ‹è¯•å®Œæˆï¼Œæœ‰æ•ˆé¢‘é“: {len(valid_channels)}/{len(tested_channels)}")
            
            # å¦‚æœæœ‰æ•ˆé¢‘é“å¤ªå°‘ï¼Œä¿ç•™åŸå§‹é¢‘é“æ•°æ®
            if len(valid_channels) < len(channels) * 0.1 and len(valid_channels) < 5:
                logger.warning(f"æœ‰æ•ˆé¢‘é“æ•°é‡è¿‡å°‘ ({len(valid_channels)}), ä½¿ç”¨åŸå§‹é¢‘é“æ•°æ®")
                # å°†åŸå§‹é¢‘é“æ•°æ®æ·»åŠ æ¨¡æ‹Ÿå»¶è¿Ÿä¿¡æ¯
                for channel in channels:
                    if 'status' not in channel:
                        channel['status'] = 'success'
                    if 'latency' not in channel:
                        # æ ¹æ®é¢‘é“åç§°ç”Ÿæˆä¸€ä¸ªä¼ªéšæœºå»¶è¿Ÿå€¼
                        name_hash = sum(ord(c) for c in channel.get('name', ''))
                        channel['latency'] = 100 + (name_hash % 900)  # 100-1000ms
                    if 'latency_tier' not in channel:
                        latency = channel.get('latency', 500)
                        if latency < 100:
                            channel['latency_tier'] = 'ultra_low'
                        elif latency < 300:
                            channel['latency_tier'] = 'low'
                        elif latency < 800:
                            channel['latency_tier'] = 'medium'
                        else:
                            channel['latency_tier'] = 'high'
                return channels
                
            return valid_channels
        except Exception as e:
            logger.error(f"å»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # è¿”å›åŸå§‹é¢‘é“æ•°æ®ï¼Œæ·»åŠ æ¨¡æ‹Ÿå»¶è¿Ÿä¿¡æ¯
            for channel in channels:
                if 'status' not in channel:
                    channel['status'] = 'success'
                if 'latency' not in channel:
                    # æ ¹æ®é¢‘é“åç§°ç”Ÿæˆä¸€ä¸ªä¼ªéšæœºå»¶è¿Ÿå€¼
                    name_hash = sum(ord(c) for c in channel.get('name', ''))
                    channel['latency'] = 100 + (name_hash % 900)  # 100-1000ms
                if 'latency_tier' not in channel:
                    latency = channel.get('latency', 500)
                    if latency < 100:
                        channel['latency_tier'] = 'ultra_low'
                    elif latency < 300:
                        channel['latency_tier'] = 'low'
                    elif latency < 800:
                        channel['latency_tier'] = 'medium'
                    else:
                        channel['latency_tier'] = 'high'
            return channels

    def _run_smart_optimization(self, channels: List[Dict]) -> Dict[str, List[Dict]]:
        """è¿è¡Œæ™ºèƒ½ä¼˜åŒ–"""
        logger.info("å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–å™¨...")

        if not channels:
            logger.warning("æ²¡æœ‰é¢‘é“æ•°æ®å¯ä¾›ä¼˜åŒ–")
            return {}

        try:
            optimizer = SmartOptimizer()
            optimized_channels = optimizer.optimize_channels(channels)

            if not optimized_channels:
                logger.warning("ä¼˜åŒ–ç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤åˆ†ç»„")
                # å¦‚æœä¼˜åŒ–ç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤åˆ†ç»„
                tier_groups = {}
                for channel in channels:
                    tier = channel.get('latency_tier', 'medium')
                    if tier not in tier_groups:
                        tier_groups[tier] = []
                    tier_groups[tier].append(channel)
                optimized_channels = tier_groups

            # ç”Ÿæˆå¢å¼ºç‰ˆæ’­æ”¾åˆ—è¡¨
            try:
                os.makedirs(self.output_dir, exist_ok=True)
                optimizer.generate_enhanced_playlist(optimized_channels, self.output_dir)
                logger.info(f"æ’­æ”¾åˆ—è¡¨ç”Ÿæˆå®Œæˆï¼Œä¿å­˜åˆ° {self.output_dir}")
            except Exception as playlist_error:
                logger.error(f"ç”Ÿæˆæ’­æ”¾åˆ—è¡¨å¤±è´¥: {playlist_error}")
                import traceback
                logger.error(traceback.format_exc())

            logger.info("æ™ºèƒ½ä¼˜åŒ–å®Œæˆ")
            return optimized_channels

        except Exception as e:
            logger.error(f"æ™ºèƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # åˆ›å»ºé»˜è®¤åˆ†ç»„
            tier_groups = {}
            for channel in channels:
                tier = channel.get('latency_tier', 'medium')
                if tier not in tier_groups:
                    tier_groups[tier] = []
                tier_groups[tier].append(channel)
            
            return tier_groups

    def _run_tvbox_optimization(self, optimized_channels: Dict[str, List[Dict]]) -> List[Dict]:
        """è¿è¡ŒTVBoxä¸“ç”¨ä¼˜åŒ–"""
        logger.info("å¯åŠ¨TVBoxä¼˜åŒ–å™¨...")

        if not optimized_channels:
            logger.warning("æ²¡æœ‰ä¼˜åŒ–é¢‘é“æ•°æ®å¯ä¾›ä½¿ç”¨")
            return []

        try:
            optimizer = TVBoxOptimizer()

            # åˆå¹¶æ‰€æœ‰ä¼˜åŒ–åçš„é¢‘é“ç”¨äºTVBoxä¼˜åŒ–
            all_channels = []
            for tier_channels in optimized_channels.values():
                all_channels.extend(tier_channels)

            if not all_channels:
                logger.warning("æ²¡æœ‰é¢‘é“æ•°æ®å¯ä¾›ä¼˜åŒ–")
                return []

            tvbox_channels = optimizer.optimize_for_tvbox(all_channels)

            # ç”ŸæˆTVBoxä¸“ç”¨æ’­æ”¾åˆ—è¡¨
            try:
                os.makedirs(self.output_dir, exist_ok=True)
                tvbox_playlist_path = os.path.join(self.output_dir, "tvbox_optimized.m3u")
                optimizer.generate_tvbox_playlist(tvbox_channels, tvbox_playlist_path)
                logger.info(f"TVBoxæ’­æ”¾åˆ—è¡¨ç”Ÿæˆå®Œæˆ: {tvbox_playlist_path}")

                # ç”ŸæˆTVBoxé…ç½®æ–‡ä»¶
                optimizer.generate_tvbox_config(tvbox_channels, self.output_dir)
                logger.info(f"TVBoxé…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
            except Exception as playlist_error:
                logger.error(f"ç”ŸæˆTVBoxæ’­æ”¾åˆ—è¡¨å¤±è´¥: {playlist_error}")
                import traceback
                logger.error(traceback.format_exc())

            logger.info(f"TVBoxä¼˜åŒ–å®Œæˆï¼Œå…± {len(tvbox_channels)} ä¸ªé¢‘é“")
            return tvbox_channels

        except Exception as e:
            logger.error(f"TVBoxä¼˜åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹é¢‘é“æ•°æ®
            all_channels = []
            for tier_channels in optimized_channels.values():
                all_channels.extend(tier_channels)
            return all_channels

    def _generate_final_output(self, optimized_channels: Dict[str, List[Dict]],
                              tvbox_channels: List[Dict]):
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
        logger.info("ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š...")

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self._generate_statistics(optimized_channels, tvbox_channels)

        stats_file = os.path.join(self.output_base, "optimization_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆREADMEæ›´æ–°
        self._generate_readme_update(stats)

        logger.info("æœ€ç»ˆè¾“å‡ºç”Ÿæˆå®Œæˆ")

    def _generate_statistics(self, optimized_channels: Dict[str, List[Dict]],
                           tvbox_channels: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ•°æ®"""
        stats = {
            "generated_at": datetime.now().isoformat(),
            "total_channels": 0,
            "tier_breakdown": {},
            "latency_stats": {},
            "quality_stats": {},
            "tvbox_optimized": len(tvbox_channels),
            "output_files": [],
            "group_stats": {},
            "protocol_stats": {}
        }

        # ç»Ÿè®¡å„ç­‰çº§é¢‘é“
        all_channels = []
        for tier, channels in optimized_channels.items():
            stats["tier_breakdown"][tier] = len(channels)
            stats["total_channels"] += len(channels)
            all_channels.extend(channels)

            if channels:
                latencies = [c.get('latency', 0) for c in channels if c.get('latency', 0) > 0]
                qualities = [c.get('quality_score', 0) for c in channels if c.get('quality_score', 0) > 0]

                if latencies:
                    stats["latency_stats"][tier] = {
                        "min": min(latencies),
                        "max": max(latencies),
                        "avg": sum(latencies) / len(latencies)
                    }

                if qualities:
                    stats["quality_stats"][tier] = {
                        "min": min(qualities),
                        "max": max(qualities),
                        "avg": sum(qualities) / len(qualities)
                    }

        # ç»Ÿè®¡åˆ†ç»„ä¿¡æ¯
        group_counts = {}
        for channel in all_channels:
            group = channel.get('group_title', 'å…¶ä»–')
            if group not in group_counts:
                group_counts[group] = 0
            group_counts[group] += 1
        
        # æŒ‰é¢‘é“æ•°é‡æ’åº
        sorted_groups = sorted(group_counts.items(), key=lambda x: x[1], reverse=True)
        for group, count in sorted_groups:
            stats["group_stats"][group] = count

        # ç»Ÿè®¡åè®®ä¿¡æ¯
        protocol_counts = {}
        for channel in all_channels:
            url = channel.get('url', '')
            protocol = 'unknown'
            if url.startswith('http'):
                if '.m3u8' in url:
                    protocol = 'hls'
                elif '.flv' in url:
                    protocol = 'flv'
                else:
                    protocol = 'http'
            elif url.startswith('rtmp'):
                protocol = 'rtmp'
            elif url.startswith('rtsp'):
                protocol = 'rtsp'
            
            if protocol not in protocol_counts:
                protocol_counts[protocol] = 0
            protocol_counts[protocol] += 1
        
        stats["protocol_stats"] = protocol_counts

        # ç»Ÿè®¡è¾“å‡ºæ–‡ä»¶
        base_output_files = [
            "aggregated_channels.json",
            "latency_test_results.json",
            "optimization_stats.json"
        ]

        playlist_output_files = [
            "iptv_ultra_low_latency.m3u",
            "iptv_low_latency.m3u",
            "iptv_medium_latency.m3u",
            "iptv_high_latency.m3u",
            "iptv_optimized_combined.m3u",
            "tvbox_optimized.m3u",
            "tvbox_config.json"
        ]

        # æ£€æŸ¥åŸºç¡€è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
        for filename in base_output_files:
            filepath = os.path.join(self.output_base, filename)
            if os.path.exists(filepath):
                stats["output_files"].append({
                    "name": filename,
                    "path": filepath,
                    "size": os.path.getsize(filepath)
                })

        # æ£€æŸ¥æ’­æ”¾åˆ—è¡¨è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
        for filename in playlist_output_files:
            filepath = os.path.join(self.output_dir, filename)
            if os.path.exists(filepath):
                stats["output_files"].append({
                    "name": filename,
                    "path": filepath,
                    "size": os.path.getsize(filepath)
                })

        # æ·»åŠ æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
        stats["execution_time"] = {
            "start_time": getattr(self, 'start_time', datetime.now().isoformat()),
            "end_time": datetime.now().isoformat()
        }

        return stats

    def _generate_readme_update(self, stats: Dict):
        """ç”ŸæˆREADMEæ›´æ–°å»ºè®®"""
        update_file = os.path.join(self.output_base, "readme_update.txt")

        try:
            with open(update_file, 'w', encoding='utf-8') as f:
                # æ ‡é¢˜å’Œç”Ÿæˆæ—¶é—´
                f.write("## IPTVæ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿæ›´æ–°æŠ¥å‘Š\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {stats['generated_at']}\n\n")

                # æ€»ä½“ç»Ÿè®¡
                f.write("### ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
                f.write(f"- æ€»é¢‘é“æ•°: {stats['total_channels']}\n")
                f.write(f"- TVBoxä¼˜åŒ–é¢‘é“æ•°: {stats['tvbox_optimized']}\n")
                
                # åˆ†çº§ç»Ÿè®¡
                f.write("\n### ğŸ“ˆ åˆ†çº§ç»Ÿè®¡\n")
                tier_names = {
                    'ultra_low': 'è¶…ä½å»¶è¿Ÿ (<100ms)',
                    'low': 'ä½å»¶è¿Ÿ (<300ms)',
                    'medium': 'ä¸­ç­‰å»¶è¿Ÿ (<800ms)',
                    'high': 'å¯æ¥å—å»¶è¿Ÿ (<2s)'
                }
                
                for tier, count in stats['tier_breakdown'].items():
                    tier_display = tier_names.get(tier, tier)
                    f.write(f"- {tier_display}: {count} ä¸ªé¢‘é“")
                    
                    # æ·»åŠ å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯
                    if tier in stats['latency_stats']:
                        latency_stats = stats['latency_stats'][tier]
                        f.write(f" (å»¶è¿Ÿ: å¹³å‡ {latency_stats['avg']:.1f}ms, æœ€ä½ {latency_stats['min']:.1f}ms)")
                    
                    f.write("\n")
                
                # åˆ†ç»„ç»Ÿè®¡
                if stats.get('group_stats'):
                    f.write("\n### ğŸ“ é¢‘é“åˆ†ç»„\n")
                    for group, count in list(stats['group_stats'].items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªåˆ†ç»„
                        f.write(f"- {group}: {count} ä¸ªé¢‘é“\n")
                    
                    if len(stats['group_stats']) > 10:
                        f.write(f"- å…¶ä»–: {sum(list(stats['group_stats'].values())[10:])} ä¸ªé¢‘é“\n")
                
                # åè®®ç»Ÿè®¡
                if stats.get('protocol_stats'):
                    f.write("\n### ğŸ”— åè®®ç»Ÿè®¡\n")
                    protocol_names = {
                        'hls': 'HLS (m3u8)',
                        'http': 'HTTP',
                        'rtmp': 'RTMP',
                        'rtsp': 'RTSP',
                        'flv': 'FLV',
                        'unknown': 'æœªçŸ¥åè®®'
                    }
                    
                    for protocol, count in stats['protocol_stats'].items():
                        protocol_display = protocol_names.get(protocol, protocol)
                        f.write(f"- {protocol_display}: {count} ä¸ªé¢‘é“\n")

                # ç”Ÿæˆæ–‡ä»¶
                f.write("\n### ğŸ’¾ ç”Ÿæˆæ–‡ä»¶\n")
                
                # æŒ‰ç±»å‹åˆ†ç»„æ–‡ä»¶
                playlist_files = []
                data_files = []
                config_files = []
                
                for file_info in stats['output_files']:
                    name = file_info['name']
                    size_kb = file_info['size'] / 1024
                    
                    if name.endswith('.m3u') or name.endswith('.m3u8'):
                        playlist_files.append((name, size_kb))
                    elif name.endswith('.json'):
                        if name == 'tvbox_config.json':
                            config_files.append((name, size_kb))
                        else:
                            data_files.append((name, size_kb))
                    else:
                        data_files.append((name, size_kb))
                
                if playlist_files:
                    f.write("#### æ’­æ”¾åˆ—è¡¨\n")
                    for name, size_kb in playlist_files:
                        f.write(f"- {name} ({size_kb:.1f} KB)\n")
                
                if data_files:
                    f.write("#### æ•°æ®æ–‡ä»¶\n")
                    for name, size_kb in data_files:
                        f.write(f"- {name} ({size_kb:.1f} KB)\n")
                
                if config_files:
                    f.write("#### é…ç½®æ–‡ä»¶\n")
                    for name, size_kb in config_files:
                        f.write(f"- {name} ({size_kb:.1f} KB)\n")

                # ä½¿ç”¨å»ºè®®
                f.write("\n### ğŸ”§ ä½¿ç”¨å»ºè®®\n")
                f.write("1. **TVBoxç”¨æˆ·**: æ¨èä½¿ç”¨ `tvbox_optimized.m3u` - åŒ…å«ä¸“ç”¨æ’­æ”¾å‚æ•°å’Œç¼“å­˜ä¼˜åŒ–\n")
                f.write("2. **è¶…ä½å»¶è¿Ÿéœ€æ±‚**: æ¨èä½¿ç”¨ `iptv_ultra_low_latency.m3u` - å»¶è¿Ÿ<100msçš„é¢‘é“\n")
                f.write("3. **é€šç”¨ç”¨æˆ·**: æ¨èä½¿ç”¨ `iptv_optimized_combined.m3u` - å„å»¶è¿Ÿç­‰çº§çš„æœ€ä½³é¢‘é“\n")
                f.write("4. **ç¨³å®šæ€§éœ€æ±‚**: æ¨èä½¿ç”¨ `iptv_medium_latency.m3u` - å»¶è¿Ÿé€‚ä¸­ä½†æ›´ç¨³å®š\n")

                # æ‰§è¡Œæ—¶é—´
                if 'execution_time' in stats:
                    start_time = stats['execution_time'].get('start_time', '')
                    end_time = stats['execution_time'].get('end_time', '')
                    if start_time and end_time:
                        try:
                            start_dt = datetime.fromisoformat(start_time)
                            end_dt = datetime.fromisoformat(end_time)
                            duration = (end_dt - start_dt).total_seconds()
                            f.write(f"\n### â­ æ‰§è¡Œä¿¡æ¯\n")
                            f.write(f"- æ€»è€—æ—¶: {duration:.1f} ç§’\n")
                        except:
                            pass

            logger.info(f"READMEæ›´æ–°å»ºè®®å·²ç”Ÿæˆ: {update_file}")
        except Exception as e:
            logger.error(f"READMEæ›´æ–°å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())

async def main():
    """ä¸»å‡½æ•°"""
    print_log("å¯åŠ¨IPTVæ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿ...")
    logger.info("å¯åŠ¨IPTVæ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿ...")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
    runner = OptimizationRunner()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print_log(f"é…ç½®ä¿¡æ¯:")
    print_log(f"- é…ç½®ç›®å½•: {runner.config_dir}")
    print_log(f"- è¾“å‡ºç›®å½•: {runner.output_base}")
    print_log(f"- æ‰¹å¤„ç†å¤§å°: {runner.batch_size}")
    print_log(f"- æµ‹è¯•æ¨¡å¼: {'å¯ç”¨' if runner.test_mode else 'ç¦ç”¨'}")
    
    logger.info(f"é…ç½®ä¿¡æ¯:")
    logger.info(f"- é…ç½®ç›®å½•: {runner.config_dir}")
    logger.info(f"- è¾“å‡ºç›®å½•: {runner.output_base}")
    logger.info(f"- æ‰¹å¤„ç†å¤§å°: {runner.batch_size}")
    logger.info(f"- æµ‹è¯•æ¨¡å¼: {'å¯ç”¨' if runner.test_mode else 'ç¦ç”¨'}")

    try:
        print_log("å¼€å§‹è¿è¡Œä¼˜åŒ–æµç¨‹...")
        await runner.run_full_optimization()
        print_log("ğŸ‰ æ‰€æœ‰ä¼˜åŒ–ä»»åŠ¡å®Œæˆ!")
        logger.info("ğŸ‰ æ‰€æœ‰ä¼˜åŒ–ä»»åŠ¡å®Œæˆ!")
        return 0
    except Exception as e:
        logger.error(f"ä¼˜åŒ–ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    print("=== IPTVæ™ºèƒ½ä¼˜åŒ–ç³»ç»Ÿå¯åŠ¨ ===")
    try:
        asyncio.run(main())
        print("=== ä¼˜åŒ–ç³»ç»Ÿæ‰§è¡Œå®Œæˆ ===")
    except Exception as e:
        print(f"=== é”™è¯¯: {e} ===")
        import traceback
        print(traceback.format_exc())
